# -*- coding: utf-8 -*-
# 1. –£–°–¢–ê–ù–û–í–ö–ê –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô –ò –ò–ú–ü–û–†–¢–´
# –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É –≤–∞—Å –µ—Å—Ç—å .env —Ñ–∞–π–ª —Å –≤–∞—à–∏–º GOOGLE_API_KEY

# !pip install instructor langchain-google-genai matplotlib networkx numpy python-dotenv scikit-learn spacy tqdm pydantic jsonref

# üöÄ –ü–ê–†–ê–õ–õ–ï–õ–ò–ó–ê–¶–ò–Ø: –¢–µ–ø–µ—Ä—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ PDF –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
# –ù–∞—Å—Ç—Ä–æ–π—Ç–µ MAX_WORKERS –≤ main –±–ª–æ–∫–µ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–æ—Ç–æ–∫–æ–≤

import networkx as nx
from pydantic import BaseModel, Field
from typing import List, Literal, Optional
import instructor
import os
import json
import hashlib
from pathlib import Path
from dotenv import load_dotenv
from tqdm import tqdm
import matplotlib.pyplot as plt
from collections import defaultdict
import concurrent.futures
import threading
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É PDF
try:
    from google import genai
    from google.genai import types
    GENAI_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è google-genai –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –¥–ª—è —á—Ç–µ–Ω–∏—è PDF")
    GENAI_AVAILABLE = False

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ Google API –∫–ª—é—á–∞
if not os.getenv('GOOGLE_API_KEY'):
    print("‚ùå –û—à–∏–±–∫–∞: GOOGLE_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω!")
    print("üîß –ü–æ–ª—É—á–∏—Ç–µ –∫–ª—é—á: https://makersuite.google.com/app/apikey")
    print("üîß –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: export GOOGLE_API_KEY=your_api_key_here")
    exit(1)

os.environ["GOOGLE_API_KEY"] = os.getenv('GOOGLE_API_KEY')
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# –ö–ª–∏–µ–Ω—Ç—ã Gemini —á–µ—Ä–µ–∑ Instructor
print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Gemini –∫–ª–∏–µ–Ω—Ç–æ–≤...")
try:
    # –ö–ª–∏–µ–Ω—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (–±—ã—Å—Ç—Ä—ã–π, –¥–µ—à–µ–≤—ã–π)
    llm_extractor_client = instructor.from_provider(
        "google/gemini-2.0-flash",
        mode=instructor.Mode.GENAI_TOOLS
    )
    
    # –ö–ª–∏–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –∫—Ä–∏—Ç–∏–∫–∏ (–º–æ—â–Ω—ã–π)
    llm_critic_client = instructor.from_provider(
        "google/gemini-2.5-flash", 
        mode=instructor.Mode.GENAI_STRUCTURED_OUTPUTS
    )
    print("‚úÖ Gemini –∫–ª–∏–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã!")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Gemini: {e}")
    print("üîß –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–∞—à GOOGLE_API_KEY")
    exit(1)

# --- 2. –ú–û–î–ï–õ–ò –î–ê–ù–ù–´–• (Pydantic) ---
# –û–ø–∏—Å—ã–≤–∞–µ–º –Ω–∞—à—É –æ–Ω—Ç–æ–ª–æ–≥–∏—é –≥—Ä–∞—Ñ–∞ + –¥–æ–±–∞–≤–ª—è–µ–º –º–æ–¥–µ–ª–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏

ConceptType = Literal["Hypothesis", "Method", "Result", "Conclusion"]
EntityType = Literal["Gene", "Protein", "Disease", "Compound", "Process"]

class MentionedEntity(BaseModel):
    name: str = Field(..., description="Normalized name of the biological entity, e.g., 'SIRT1', 'mTOR'")
    type: EntityType

class ScientificConcept(BaseModel):
    concept_type: ConceptType
    statement: str
    mentioned_entities: List[MentionedEntity] = Field(default_factory=list)

class ExtractedKnowledge(BaseModel):
    paper_id: str
    concepts: List[ScientificConcept]

# –£–õ–£–ß–®–ï–ù–ù–´–ï –ú–û–î–ï–õ–ò –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π
class Critique(BaseModel):
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç –ê–≥–µ–Ω—Ç–∞-–ö—Ä–∏—Ç–∏–∫–∞."""
    is_interesting: bool = Field(..., description="Does this direction pass the basic sanity check for being interesting?")
    novelty_score: float = Field(..., description="Score from 0-10 for scientific novelty. 10 is a completely new paradigm.")
    impact_score: float = Field(..., description="Score from 0-10 for potential impact. 10 could lead to a Nobel prize.")
    feasibility_score: float = Field(..., description="Score from 0-10 for technical feasibility. 10 can be done with current tech in <1 year.")
    final_score: float = Field(..., description="A final weighted score (0.5*impact + 0.3*novelty + 0.2*feasibility).")
    strengths: List[str] = Field(..., description="Bulleted list of key strengths.")
    weaknesses: List[str] = Field(..., description="Bulleted list of critical weaknesses and potential risks.")
    recommendation: Literal["Strongly Recommend", "Consider", "Reject"]

class PrioritizedDirection(BaseModel):
    rank: int
    title: str
    description: str
    critique: Critique
    supporting_papers: List[str]

# --- –ù–û–í–´–ï –ö–õ–ê–°–°–´ –î–õ–Ø PDF –ò –ö–≠–®–ò–†–û–í–ê–ù–ò–Ø ---

class SimplePDFReader:
    def __init__(self):
        if GENAI_AVAILABLE:
            self.client = genai.Client(api_key=os.getenv('GOOGLE_API_KEY'))
        else:
            self.client = None
    
    def read_pdf(self, pdf_path: str) -> str:
        if not self.client:
            return "PDF reader –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ google-genai"
        
        try:
            pdf_path = Path(pdf_path)
            pdf_data = pdf_path.read_bytes()
            
            prompt = """–ò–∑–≤–ª–µ–∫–∏ –í–ï–°–¨ —Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞—É—á–Ω–æ–≥–æ PDF. 
            –í–∫–ª—é—á–∏: –≤–≤–µ–¥–µ–Ω–∏–µ, –º–µ—Ç–æ–¥—ã, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –æ–±—Å—É–∂–¥–µ–Ω–∏–µ, –∑–∞–∫–ª—é—á–µ–Ω–∏–µ.
            –ù–ï —Å—É–º–º–∏—Ä—É–π - –Ω—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç!"""
            
            response = self.client.models.generate_content(
                model="gemini-2.0-flash",
                contents=[
                    types.Part.from_bytes(data=pdf_data, mime_type='application/pdf'),
                    prompt
                ]
            )
            return response.text
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF {pdf_path}: {e}")
            return ""

class CacheManager:
    def __init__(self, cache_dir: str = "cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        self.pdf_cache_file = self.cache_dir / "pdf_texts.json"
        self.pdf_cache = self._load_cache()
        # –î–æ–±–∞–≤–ª—è–µ–º –±–ª–æ–∫–∏—Ä–æ–≤–∫—É –¥–ª—è –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        self._lock = threading.Lock()
    
    def _load_cache(self):
        try:
            if self.pdf_cache_file.exists():
                with open(self.pdf_cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except:
            pass
        return {}
    
    def _save_cache(self):
        with self._lock:  # –ë–ª–æ–∫–∏—Ä—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–ª—è –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
            with open(self.pdf_cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.pdf_cache, f, ensure_ascii=False, indent=2)
    
    def get_pdf_text(self, pdf_path: str) -> str:
        file_key = f"{Path(pdf_path).name}_{os.path.getmtime(pdf_path)}"
        with self._lock:  # –ë–ª–æ–∫–∏—Ä—É–µ–º —á—Ç–µ–Ω–∏–µ –¥–ª—è –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
            return self.pdf_cache.get(file_key, "")
    
    def save_pdf_text(self, pdf_path: str, text: str):
        file_key = f"{Path(pdf_path).name}_{os.path.getmtime(pdf_path)}"
        with self._lock:  # –ë–ª–æ–∫–∏—Ä—É–µ–º –∑–∞–ø–∏—Å—å –¥–ª—è –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
            self.pdf_cache[file_key] = text
        self._save_cache()

# --- 3. –ê–î–ê–ü–¢–ò–†–û–í–ê–ù–ù–´–ô –ö–õ–ê–°–° –î–õ–Ø –ì–†–ê–§–ê –ó–ù–ê–ù–ò–ô ---

class ScientificKnowledgeGraph:
    def __init__(self):
        self.graph = nx.DiGraph()
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É PDF –∏ –∫—ç—à–∞
        self.pdf_reader = SimplePDFReader()
        self.cache = CacheManager()

    def save_graph(self, filepath: str = "knowledge_graph.graphml"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≥—Ä–∞—Ñ –≤ —Ñ–∞–π–ª GraphML."""
        try:
            filepath = Path(filepath)
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            filepath.parent.mkdir(parents=True, exist_ok=True)
            
            nx.write_graphml(self.graph, filepath)
            print(f"üíæ –ì—Ä–∞—Ñ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {filepath}")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∞: {e}")
            return False

    def load_graph(self, filepath: str = "knowledge_graph.graphml") -> bool:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≥—Ä–∞—Ñ –∏–∑ —Ñ–∞–π–ª–∞ GraphML."""
        try:
            filepath = Path(filepath)
            if not filepath.exists():
                print(f"üìÅ –§–∞–π–ª –≥—Ä–∞—Ñ–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {filepath}")
                return False
            
            self.graph = nx.read_graphml(filepath)
            print(f"‚úÖ –ì—Ä–∞—Ñ –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ —Ñ–∞–π–ª–∞: {filepath}")
            print(f"   üìä –£–∑–ª–æ–≤: {self.graph.number_of_nodes()}, –†—ë–±–µ—Ä: {self.graph.number_of_edges()}")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≥—Ä–∞—Ñ–∞: {e}")
            return False

    def get_graph_stats(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≥—Ä–∞—Ñ–∞."""
        if not self.graph:
            return "–ì—Ä–∞—Ñ –ø—É—Å—Ç"
        
        stats = {
            'nodes': self.graph.number_of_nodes(),
            'edges': self.graph.number_of_edges(),
            'papers': len([n for n, d in self.graph.nodes(data=True) if d.get('type') == 'Paper']),
            'hypotheses': len([n for n, d in self.graph.nodes(data=True) if d.get('type') == 'Hypothesis']),
            'methods': len([n for n, d in self.graph.nodes(data=True) if d.get('type') == 'Method']),
            'results': len([n for n, d in self.graph.nodes(data=True) if d.get('type') == 'Result']),
            'conclusions': len([n for n, d in self.graph.nodes(data=True) if d.get('type') == 'Conclusion']),
            'entities': len([n for n, d in self.graph.nodes(data=True) if d.get('type') == 'Entity'])
        }
        return stats

    def _extract_scientific_concepts(self, paper_id: str, text: str) -> ExtractedKnowledge:
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ –µ—Å–ª–∏ –æ–Ω —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π
        text_for_prompt = text[:80000] + ("..." if len(text) > 80000 else "")
        
        prompt_text = f"""
        You are an expert in scientific research methodology and bioinformatics.
        
        TASK: Analyze the following FULL scientific paper text and extract its core components.
        
        IMPORTANT DISTINCTIONS:
        - Hypothesis: A testable prediction or proposed explanation (often starts with "we hypothesize", "we propose", "we test the hypothesis")
        - Method: The experimental technique or approach used (e.g., "using CRISPR", "via flow cytometry", "mass spectrometry")  
        - Result: The actual findings or observations from experiments (e.g., "we observed", "showed", "revealed")
        - Conclusion: Final interpretations or implications drawn from results (e.g., "we conclude", "this confirms")
        
        For each component, identify all mentioned biological entities (Genes like SIRT1, Proteins like mTOR, Diseases, Compounds like Rapamycin, Processes like senescence).
        
        BE PRECISE: A hypothesis without corresponding results in the same paper should remain unconnected.

        FULL PAPER TEXT: "{text_for_prompt}"
        
        Paper ID: {paper_id}
        """
        
        try:
            knowledge = llm_extractor_client.chat.completions.create(
                messages=[{"role": "user", "content": prompt_text}],
                response_model=ExtractedKnowledge
            )
            knowledge.paper_id = paper_id  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π paper_id
            return knowledge
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ –¥–ª—è {paper_id}: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            return ExtractedKnowledge(paper_id=paper_id, concepts=[])

    def _process_single_document(self, paper_id, doc_data):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ (–¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏–∏)."""
        text = doc_data['full_text']
        year = doc_data.get('year', 2024)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ü–µ–ø—Ç—ã
        extracted_knowledge = self._extract_scientific_concepts(paper_id, text)
        
        print(f"  üìÑ {paper_id}: –Ω–∞–π–¥–µ–Ω–æ {len(extracted_knowledge.concepts)} –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤")
        
        return paper_id, text, year, extracted_knowledge

    def build_graph(self, documents: dict, max_workers=4):
        print(f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ –∏–∑ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–ø–æ—Ç–æ–∫–æ–≤: {max_workers})")
        
        # –°–Ω–∞—á–∞–ª–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã
        all_results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
            future_to_paper = {
                executor.submit(self._process_single_document, paper_id, doc_data): paper_id 
                for paper_id, doc_data in documents.items()
            }
            
            # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
            for future in tqdm(concurrent.futures.as_completed(future_to_paper), 
                             total=len(documents), desc="–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤"):
                paper_id = future_to_paper[future]
                try:
                    result = future.result()
                    all_results.append(result)
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {paper_id}: {e}")
        
        print("üèóÔ∏è –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ –∏–∑ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤...")
        # –¢–µ–ø–µ—Ä—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —Å—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ (–∑–¥–µ—Å—å –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ–∂–Ω–µ–µ –∏–∑-–∑–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≥—Ä–∞—Ñ–∞)
        for paper_id, text, year, extracted_knowledge in tqdm(all_results, desc="–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞"):
            # –î–æ–±–∞–≤–ª—è–µ–º —É–∑–µ–ª –¥–ª—è —Å—Ç–∞—Ç—å–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            self.graph.add_node(paper_id, type='Paper', content=text[:500], year=year)
            
            # –í—ã–≤–æ–¥–∏–º –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            for concept in extracted_knowledge.concepts:
                print(f"    - {concept.concept_type}: {concept.statement[:50]}...")
            
            for concept in extracted_knowledge.concepts:
                concept_id = f"{paper_id}_{concept.concept_type}_{hash(concept.statement)}"
                self.graph.add_node(concept_id, 
                                  type=concept.concept_type, 
                                  content=concept.statement, 
                                  statement=concept.statement,
                                  paper_id=paper_id)
                self.graph.add_edge(paper_id, concept_id, type='CONTAINS')
                
                for entity in concept.mentioned_entities:
                    entity_id = f"{entity.type}_{entity.name.upper()}"
                    if not self.graph.has_node(entity_id):
                        self.graph.add_node(entity_id, 
                                          type='Entity', 
                                          entity_type=entity.type, 
                                          name=entity.name.upper())
                    self.graph.add_edge(concept_id, entity_id, type='MENTIONS')
            
            # ‚ùå –£–ë–ò–†–ê–ï–ú –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ - –ø—É—Å—Ç—å LLM —Å–∞–º –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–≤—è–∑–∏
            # –ù–ï —Å–æ–∑–¥–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∏ –º–µ–∂–¥—É –≤—Å–µ–º–∏ –≥–∏–ø–æ—Ç–µ–∑–∞–º–∏ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏!

    def visualize_graph(self):
        # –ü—Ä–æ—Å—Ç–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        plt.figure(figsize=(12, 8))
        pos = nx.spring_layout(self.graph, k=1, iterations=50)
        
        # –†–∞–∑–Ω—ã–µ —Ü–≤–µ—Ç–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —É–∑–ª–æ–≤
        colors = {'Paper': 'lightblue', 'Hypothesis': 'lightgreen', 
                 'Method': 'lightyellow', 'Result': 'lightcoral', 
                 'Conclusion': 'lightpink', 'Entity': 'lightgray'}
        
        for node_type in colors:
            nodes = [n for n, data in self.graph.nodes(data=True) if data.get('type') == node_type]
            nx.draw_networkx_nodes(self.graph, pos, nodelist=nodes, 
                                 node_color=colors[node_type], 
                                 node_size=300, alpha=0.8)
        
        nx.draw_networkx_edges(self.graph, pos, alpha=0.5, arrows=True)
        plt.title("Scientific Knowledge Graph")
        plt.axis('off')
        plt.show()

# --- 4. –£–õ–£–ß–®–ï–ù–ù–´–ô –ù–ê–£–ß–ù–´–ô –ê–ù–ê–õ–ò–¢–ò–ö ---

class ResearchAnalyst:
    def __init__(self, knowledge_graph: ScientificKnowledgeGraph):
        self.graph = knowledge_graph.graph

    # --- –ú–ï–¢–û–î–´ –î–õ–Ø –§–ê–ó–´ –†–ê–°–•–û–ñ–î–ï–ù–ò–Ø (DIVERGENT) ---
    
    def _generate_directions_from_white_spots(self) -> List[dict]:
        """–ê–Ω–∞–ª–∏—Ç–∏–∫–∞ ‚Ññ1: –ü–æ–∏—Å–∫ "–±–µ–ª—ã—Ö –ø—è—Ç–µ–Ω" - –≥–∏–ø–æ—Ç–µ–∑ –±–µ–∑ –ø—Ä—è–º—ã—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤."""
        directions = []
        
        # üêõ –û–¢–õ–ê–î–ö–ê: –ø–æ–∫–∞–∂–µ–º –≤—Å–µ –≥–∏–ø–æ—Ç–µ–∑—ã
        all_hypotheses = [(node_id, data) for node_id, data in self.graph.nodes(data=True) 
                         if data.get('type') == 'Hypothesis']
        print(f"  üîç –ù–∞–π–¥–µ–Ω–æ –≥–∏–ø–æ—Ç–µ–∑: {len(all_hypotheses)}")
        
        for node_id, data in all_hypotheses:
            successors = list(self.graph.successors(node_id))
            has_results = any(self.graph.nodes[n].get('type') == 'Result' 
                            for n in successors)
            
            print(f"    - –ì–∏–ø–æ—Ç–µ–∑–∞ {data.get('paper_id')}: '{data.get('statement', 'N/A')[:30]}...'")
            print(f"      –°–≤—è–∑–µ–π: {len(successors)}, –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {has_results}")
            
            if not has_results:
                directions.append({
                    "type": "White Spot",
                    "title": f"Validation of Hypothesis from paper {data.get('paper_id')}",
                    "description": f"The paper {data.get('paper_id')} states the hypothesis '{data.get('content', data.get('statement', 'N/A'))}', but does not provide direct experimental results to support it within the abstract. This represents a 'white spot' and a clear opportunity for experimental validation.",
                    "supporting_papers": [data.get('paper_id')]
                })
        
        print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ –±–µ–ª—ã—Ö –ø—è—Ç–µ–Ω: {len(directions)}")
        return directions

    def _generate_directions_from_bridges(self) -> List[dict]:
        """–ê–Ω–∞–ª–∏—Ç–∏–∫–∞ ‚Ññ2: –ü–æ–∏—Å–∫ "–º–µ–∂–¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞—Ä–Ω—ã—Ö –º–æ—Å—Ç–æ–≤" - –æ–±—â–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –≤ —Ä–∞–∑–Ω—ã—Ö —Å—Ç–∞—Ç—å—è—Ö."""
        directions = []
        entity_papers = defaultdict(set)
        
        for node_id, data in self.graph.nodes(data=True):
            if data.get('type') == 'Entity':
                for predecessor in self.graph.predecessors(node_id):
                    paper_id = self.graph.nodes[predecessor].get('paper_id')
                    if paper_id:
                        entity_papers[data.get('name')].add(paper_id)
        
        for entity, papers in entity_papers.items():
            if len(papers) > 1:
                directions.append({
                    "type": "Bridge",
                    "title": f"Investigate the Cross-Disciplinary Role of {entity}",
                    "description": f"The entity '{entity}' is mentioned in several potentially disconnected research areas across papers {list(papers)}. This suggests an unexplored common mechanism that could be a novel research direction, bridging these fields.",
                    "supporting_papers": list(papers)
                })
        return directions

    def _generate_directions_from_new_methods(self) -> List[dict]:
        """–ê–Ω–∞–ª–∏—Ç–∏–∫–∞ ‚Ññ3: –ü–æ–∏—Å–∫ "–Ω–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –ø—Ä–æ–±–ª–µ–º"."""
        directions = []
        # –ù–∞—Ö–æ–¥–∏–º —Å–∞–º—ã–π —Å–≤–µ–∂–∏–π –≥–æ–¥ –≤ –Ω–∞—à–µ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö
        latest_year = max((data.get('year', 0) for n, data in self.graph.nodes(data=True) 
                          if data.get('type') == 'Paper'), default=2024)
        
        for node_id, data in self.graph.nodes(data=True):
            if (data.get('type') == 'Method' and 
                self.graph.nodes[data.get('paper_id', '')].get('year') == latest_year):
                
                for successor in self.graph.successors(node_id):
                    if self.graph.nodes[successor].get('type') == 'Entity':
                        entity_name = self.graph.nodes[successor].get('name')
                        directions.append({
                            "type": "New Tool, Old Problem",
                            "title": f"Apply Novel Method to Problems Involving {entity_name}",
                            "description": f"A recent paper ({data.get('paper_id')}, {latest_year}) introduced a new method: '{data.get('content', data.get('statement', 'N/A'))}'. This method was used in the context of '{entity_name}'. There is a significant opportunity to apply this state-of-the-art method to other, well-established problems where '{entity_name}' is a key factor.",
                            "supporting_papers": [data.get('paper_id')]
                        })
        return directions

    def generate_research_directions(self) -> List[dict]:
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ —Ñ–∞–∑—ã —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è: —Å–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ –∏–¥–µ–∏."""
        all_directions = []
        all_directions.extend(self._generate_directions_from_white_spots())
        all_directions.extend(self._generate_directions_from_bridges())
        all_directions.extend(self._generate_directions_from_new_methods())
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
        unique_directions = {d['title']: d for d in all_directions}.values()
        return list(unique_directions)

    # --- –ú–ï–¢–û–î –î–õ–Ø –§–ê–ó–´ –°–•–û–ñ–î–ï–ù–ò–Ø (CONVERGENT) ---

    def _critique_single_direction(self, direction: dict) -> dict:
        """–ö—Ä–∏—Ç–∏–∫—É–µ—Ç –æ–¥–Ω–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ (–¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏–∏)."""
        PROMPT_CRITIC = """
# –†–û–õ–¨
–¢—ã ‚Äî —Ü–∏–Ω–∏—á–Ω—ã–π, –Ω–æ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤—ã–π, –≤—Å–µ–º–∏—Ä–Ω–æ –∏–∑–≤–µ—Å—Ç–Ω—ã–π –Ω–∞—É—á–Ω—ã–π —Ä–µ—Ü–µ–Ω–∑–µ–Ω—Ç –¥–ª—è –∂—É—Ä–Ω–∞–ª–∞ 'Nature'. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –±–µ—Å–ø–æ—â–∞–¥–Ω–æ, –Ω–æ –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–µ –Ω–∞—É—á–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ. –¢—ã –∏—â–µ—à—å –Ω–∞—Å—Ç–æ—è—â–∏–π –ø—Ä–æ—Ä—ã–≤, –∞ –Ω–µ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è.

# –ó–ê–î–ê–ß–ê
–û—Ü–µ–Ω–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è. –¢–≤–æ–π –≤–µ—Ä–¥–∏–∫—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω –∏ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Ç—Ä–µ—Ö —Å—Ç–æ–ª–ø–∞—Ö: –ù–æ–≤–∏–∑–Ω–∞, –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –í–ª–∏—è–Ω–∏–µ –∏ –†–µ–∞–ª–∏–∑—É–µ–º–æ—Å—Ç—å. –ù–µ –ø–æ–¥–¥–∞–≤–∞–π—Å—è –Ω–∞ –∫—Ä–∞—Å–∏–≤—ã–µ —Å–ª–æ–≤–∞, —Å–º–æ—Ç—Ä–∏ –≤ —Å—É—Ç—å.

# –ü–†–ï–î–õ–û–ñ–ï–ù–ù–û–ï –ù–ê–ü–†–ê–í–õ–ï–ù–ò–ï:
"{description}"

# –ò–ù–°–¢–†–£–ö–¶–ò–ò –ü–û –û–¶–ï–ù–ö–ï:
1.  **–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∏–Ω—Ç–µ—Ä–µ—Å (is_interesting):** –≠—Ç–æ –≤–æ–æ–±—â–µ –∏–º–µ–µ—Ç —Å–º—ã—Å–ª? –ï—Å–ª–∏ –∏–¥–µ—è –∞–±—Å—É—Ä–¥–Ω–∞ –∏–ª–∏ —Ç—Ä–∏–≤–∏–∞–ª—å–Ω–∞, —Å—Ä–∞–∑—É —Å—Ç–∞–≤—å `false`.
2.  **–ù–æ–≤–∏–∑–Ω–∞ (novelty_score):** –≠—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —á—Ç–æ-—Ç–æ –Ω–æ–≤–æ–µ, –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ—É–ø–∞–∫–æ–≤–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∏–¥–µ–π? (10 = –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞, 1 = –æ—á–µ—Ä–µ–¥–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–æ BERT).
3.  **–í–ª–∏—è–Ω–∏–µ (impact_score):** –ï—Å–ª–∏ —ç—Ç–æ —Å—Ä–∞–±–æ—Ç–∞–µ—Ç, —ç—Ç–æ –∏–∑–º–µ–Ω–∏—Ç –º–∏—Ä –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–∏—Ç +0.1% –∫ –∫–∞–∫–æ–º—É-—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫—É? (10 = –ù–æ–±–µ–ª–µ–≤—Å–∫–∞—è –ø—Ä–µ–º–∏—è, 1 = –Ω–∏–∫—Ç–æ –Ω–µ –∑–∞–º–µ—Ç–∏—Ç).
4.  **–†–µ–∞–ª–∏–∑—É–µ–º–æ—Å—Ç—å (feasibility_score):** –≠—Ç–æ –º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–µ–≥–æ–¥–Ω—è –∏–ª–∏ —ç—Ç–æ –Ω–∞—É—á–Ω–∞—è —Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫–∞ –Ω–∞ 50 –ª–µ—Ç –≤–ø–µ—Ä–µ–¥? (10 = –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –∑–∞ –≥–æ–¥ –≤ –∞—Å–ø–∏—Ä–∞–Ω—Ç—É—Ä–µ, 1 = —Ç—Ä–µ–±—É–µ—Ç –ø–æ—Å—Ç—Ä–æ–π–∫–∏ –º–∞—à–∏–Ω—ã –≤—Ä–µ–º–µ–Ω–∏).
5.  **–ò—Ç–æ–≥–æ–≤–∞—è –û—Ü–µ–Ω–∫–∞ (final_score):** –†–∞—Å—Å—á–∏—Ç–∞–π –∫–∞–∫ `0.5*impact + 0.3*novelty + 0.2*feasibility`.
6.  **–°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã (strengths):** 1-2 –ø—É–Ω–∫—Ç–∞, –∑–∞ —á—Ç–æ –º–æ–∂–Ω–æ –ø–æ—Ö–≤–∞–ª–∏—Ç—å.
7.  **–°–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã (weaknesses):** 1-2 –ø—É–Ω–∫—Ç–∞, –≥–¥–µ –≥–ª–∞–≤–Ω—ã–µ —Ä–∏—Å–∫–∏ –∏ –ø—Ä–æ–±–ª–µ–º—ã.
8.  **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è (recommendation):** 'Strongly Recommend' (–µ—Å–ª–∏ final_score > 7.5), 'Consider' (–µ—Å–ª–∏ final_score > 5.0), 'Reject' (–≤–æ –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–ª—É—á–∞—è—Ö).

–¢–≤–æ–π –æ—Ç–≤–µ—Ç –î–û–õ–ñ–ï–ù –±—ã—Ç—å —Ç–æ–ª—å–∫–æ JSON-–æ–±—ä–µ–∫—Ç–æ–º, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç Pydantic-—Å—Ö–µ–º–µ Critique.
"""
        
        try:
            critique = llm_critic_client.chat.completions.create(
                messages=[{"role": "user", "content": PROMPT_CRITIC.format(description=direction['description'])}],
                response_model=Critique
            )
            
            if critique.is_interesting:
                direction['critique'] = critique
                return direction
            else:
                return None
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è '{direction['title']}': {e}")
            return None

    def critique_and_prioritize(self, directions: List[dict], max_workers=4) -> List[PrioritizedDirection]:
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ —Ñ–∞–∑—ã —Å—Ö–æ–∂–¥–µ–Ω–∏—è: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ê–≥–µ–Ω—Ç–∞-–ö—Ä–∏—Ç–∏–∫–∞ —Å –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏–µ–π."""
        print(f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –∫—Ä–∏—Ç–∏–∫—É {len(directions)} –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π (–ø–æ—Ç–æ–∫–æ–≤: {max_workers})")
        
        critiqued_directions = []
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
            future_to_direction = {
                executor.submit(self._critique_single_direction, direction): direction 
                for direction in directions
            }
            
            # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
            for future in tqdm(concurrent.futures.as_completed(future_to_direction), 
                             total=len(directions), desc="–ö—Ä–∏—Ç–∏–∫–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π"):
                try:
                    result = future.result()
                    if result is not None:
                        critiqued_directions.append(result)
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è: {e}")
        
        sorted_directions = sorted(critiqued_directions, key=lambda x: x['critique'].final_score, reverse=True)
        
        final_ranking = [
            PrioritizedDirection(
                rank=i + 1,
                title=direction['title'],
                description=direction['description'],
                critique=direction['critique'],
                supporting_papers=direction['supporting_papers']
            )
            for i, direction in enumerate(sorted_directions)
        ]
        return final_ranking

    def save_report(self, prioritized_list: List[PrioritizedDirection], filepath: str = "research_report.json"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –≤ JSON —Ñ–∞–π–ª."""
        try:
            filepath = Path(filepath)
            filepath.parent.mkdir(parents=True, exist_ok=True)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            report_data = {
                "timestamp": datetime.now().isoformat(),
                "total_directions": len(prioritized_list),
                "directions": []
            }
            
            for direction in prioritized_list:
                direction_data = {
                    "rank": direction.rank,
                    "title": direction.title,
                    "description": direction.description,
                    "supporting_papers": direction.supporting_papers,
                    "critique": {
                        "is_interesting": direction.critique.is_interesting,
                        "novelty_score": direction.critique.novelty_score,
                        "impact_score": direction.critique.impact_score,
                        "feasibility_score": direction.critique.feasibility_score,
                        "final_score": direction.critique.final_score,
                        "strengths": direction.critique.strengths,
                        "weaknesses": direction.critique.weaknesses,
                        "recommendation": direction.critique.recommendation
                    }
                }
                report_data["directions"].append(direction_data)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            print(f"üíæ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {filepath}")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {e}")
            return False

# --- 5. –§–£–ù–ö–¶–ò–Ø –ó–ê–ì–†–£–ó–ö–ò –î–ê–ù–ù–´–• ---

def process_single_pdf(pdf_file, cache, pdf_reader):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω PDF —Ñ–∞–π–ª (–¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏–∏)."""
    paper_id = f"PDF_{pdf_file.stem}"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    if cache:
        cached_text = cache.get_pdf_text(str(pdf_file))
        if cached_text:
            print(f"  üìÅ {paper_id}: –∏–∑ –∫—ç—à–∞")
            return paper_id, cached_text, 2024
        else:
            print(f"  üîÑ {paper_id}: —á–∏—Ç–∞–µ–º PDF...")
            full_text = pdf_reader.read_pdf(str(pdf_file))
            if full_text:
                cache.save_pdf_text(str(pdf_file), full_text)
            return paper_id, full_text, 2024
    else:
        full_text = pdf_reader.read_pdf(str(pdf_file))
        return paper_id, full_text, 2024

def load_documents(pdf_folder="downloaded_pdfs", use_cache=True, max_workers=4):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç PDF —Ñ–∞–π–ª—ã –∏–∑ –ø–∞–ø–∫–∏ –∏–ª–∏ JSON —Ñ–∞–π–ª."""
    
    # –ò—â–µ–º –ø–∞–ø–∫—É —Å PDF –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
    possible_paths = [
        Path(pdf_folder),  # –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ç–µ–∫—É—â–µ–π –ø–∞–ø–∫–∏
        Path("downloaded_pdfs"),  # –ø—Ä—è–º–æ downloaded_pdfs
        Path(".") / "downloaded_pdfs",  # –≤ —Ç–µ–∫—É—â–µ–π –ø–∞–ø–∫–µ
        Path("..") / pdf_folder,  # –Ω–∞ —É—Ä–æ–≤–µ–Ω—å –≤—ã—à–µ
    ]
    
    pdf_path = None
    for path in possible_paths:
        print(f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Ç—å: {path.absolute()} - —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {path.exists()}, –ø–∞–ø–∫–∞: {path.is_dir() if path.exists() else 'N/A'}")
        if path.exists() and path.is_dir():
            pdf_path = path
            break
    
    if pdf_path:
        print(f"üìÅ –ù–∞–π–¥–µ–Ω–∞ –ø–∞–ø–∫–∞ —Å PDF: {pdf_path}")
        documents = {}
        pdf_files = list(pdf_path.glob("*.pdf"))
        
        cache = CacheManager() if use_cache else None
        pdf_reader = SimplePDFReader()
        
        print(f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É {len(pdf_files)} PDF —Ñ–∞–π–ª–æ–≤ (–ø–æ—Ç–æ–∫–æ–≤: {max_workers})")
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ PDF —Ñ–∞–π–ª–æ–≤
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
            future_to_pdf = {
                executor.submit(process_single_pdf, pdf_file, cache, pdf_reader): pdf_file 
                for pdf_file in pdf_files
            }
            
            # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
            for future in tqdm(concurrent.futures.as_completed(future_to_pdf), 
                             total=len(pdf_files), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ PDF —Ñ–∞–π–ª–æ–≤"):
                pdf_file = future_to_pdf[future]
                try:
                    paper_id, full_text, year = future.result()
                    if full_text:
                        documents[paper_id] = {
                            "full_text": full_text,
                            "year": year
                        }
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {pdf_file}: {e}")
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} PDF —Ñ–∞–π–ª–æ–≤")
        return documents
    
    # –ï—Å–ª–∏ –ø–∞–ø–∫–∏ –Ω–µ—Ç, –ø—Ä–æ–±—É–µ–º JSON (—Å—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞)
    try:
        with open("pubmed_corpus.json", 'r', encoding='utf-8') as f:
            data = json.load(f)
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç –≤ –Ω–æ–≤—ã–π
            converted = {}
            for paper_id, doc_data in data.items():
                converted[paper_id] = {
                    "full_text": doc_data.get('abstract', ''),
                    "year": doc_data.get('year', 2024)
                }
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(converted)} —Å—Ç–∞—Ç–µ–π –∏–∑ JSON")
            return converted
    except FileNotFoundError:
        print("–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ")
        return {
            "TEST:1": {
                "full_text": "Here we test the hypothesis that SIRT1 activation extends lifespan. Using CRISPR-Cas9 as our primary method, we observed a 20% increase in lifespan in C. elegans. We conclude that SIRT1 is a key longevity gene.",
                "year": 2023
            },
            "TEST:2": {
                "full_text": "The role of mTOR in aging is well-established. We hypothesized that inhibiting mTOR with Rapamycin would reduce cellular senescence. Our results, obtained via flow cytometry, showed a significant decrease in senescent cells. This confirms the link between mTOR and senescence.",
                "year": 2022
            },
            "TEST:3": {
                "full_text": "This paper explores if SIRT1 is involved in cellular metabolism. We propose that it is a master regulator. However, our experiments using mass spectrometry did not yield conclusive results connecting it directly to glucose uptake.",
                "year": 2021
            },
            "TEST:4": {
                "full_text": "We investigated the protein landscape of senescent cells using a novel single-cell proteomics method. Our analysis revealed that mTOR signaling is consistently upregulated. We hypothesize this is a core driver of the senescent phenotype.",
                "year": 2024
            }
        }

# --- 6. –ì–õ–ê–í–ù–´–ô –ò–°–ü–û–õ–ù–Ø–ï–ú–´–ô –ë–õ–û–ö ---

if __name__ == '__main__':
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    GRAPH_FILE = "longevity_knowledge_graph.graphml"
    REPORT_FILE = "research_report.json"  # –§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞
    FORCE_REBUILD = False  # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ True –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∞
    MAX_WORKERS = 4  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    
    print("üß¨ --- LONGEVITY RESEARCH GRAPH ANALYZER ---")
    
    # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
    skg = ScientificKnowledgeGraph()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –∑–∞–≥—Ä—É–∂–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –≥—Ä–∞—Ñ
    graph_exists = Path(GRAPH_FILE).exists()
    
    if graph_exists and not FORCE_REBUILD:
        print(f"üìÅ –ù–∞–π–¥–µ–Ω —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π –≥—Ä–∞—Ñ: {GRAPH_FILE}")
        if skg.load_graph(GRAPH_FILE):
            # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∞
            stats = skg.get_graph_stats()
            print(f"   üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥—Ä–∞—Ñ–∞:")
            print(f"      ‚Ä¢ –°—Ç–∞—Ç—å–∏: {stats['papers']}")
            print(f"      ‚Ä¢ –ì–∏–ø–æ—Ç–µ–∑—ã: {stats['hypotheses']}")
            print(f"      ‚Ä¢ –ú–µ—Ç–æ–¥—ã: {stats['methods']}")
            print(f"      ‚Ä¢ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {stats['results']}")
            print(f"      ‚Ä¢ –ó–∞–∫–ª—é—á–µ–Ω–∏—è: {stats['conclusions']}")
            print(f"      ‚Ä¢ –°—É—â–Ω–æ—Å—Ç–∏: {stats['entities']}")
        else:
            print("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≥—Ä–∞—Ñ–∞. –ù–∞—á–∏–Ω–∞–µ–º –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å –Ω—É–ª—è.")
            graph_exists = False
    
    # –ï—Å–ª–∏ –≥—Ä–∞—Ñ–∞ –Ω–µ—Ç –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å - —Å—Ç—Ä–æ–∏–º –Ω–æ–≤—ã–π
    if not graph_exists or FORCE_REBUILD:
        if FORCE_REBUILD:
            print("üîÑ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∞...")
        else:
            print("üìÅ –°–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π –≥—Ä–∞—Ñ –Ω–µ –Ω–∞–π–¥–µ–Ω. –ù–∞—á–∏–Ω–∞—é –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å –Ω—É–ª—è.")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        documents = load_documents(pdf_folder="downloaded_pdfs", max_workers=MAX_WORKERS)
        
        if not documents:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏!")
            exit(1)
        
        print("üß¨ --- Stage 1: Building the Knowledge Graph ---")
        skg.build_graph(documents, max_workers=MAX_WORKERS)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ
        if skg.save_graph(GRAPH_FILE):
            stats = skg.get_graph_stats()
            print(f"‚úÖ –ì—Ä–∞—Ñ –ø–æ—Å—Ç—Ä–æ–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {stats['nodes']} —É–∑–ª–æ–≤, {stats['edges']} —Ä—ë–±–µ—Ä")
        else:
            print("‚ö†Ô∏è –ì—Ä–∞—Ñ –ø–æ—Å—Ç—Ä–æ–µ–Ω, –Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å")
    
    print("\nüî¨ --- Stage 2: Analysis and Prioritization ---")
    analyst = ResearchAnalyst(skg)

    print("\n   üåü -> Divergent Phase: Generating raw research directions...")
    raw_directions = analyst.generate_research_directions()
    print(f"   ‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(raw_directions)} –∏—Å—Ö–æ–¥–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π.")
    
    if raw_directions:
        print("   üéØ -> Convergent Phase: Critiquing and prioritizing directions...")
        prioritized_list = analyst.critique_and_prioritize(raw_directions, max_workers=MAX_WORKERS)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç –≤ —Ñ–∞–π–ª
        if analyst.save_report(prioritized_list, REPORT_FILE):
            print(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {REPORT_FILE}")
            # –ö—Ä–∞—Ç–∫–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—Ç—á–µ—Ç–∞
            recommendations = {"Strongly Recommend": 0, "Consider": 0, "Reject": 0}
            for direction in prioritized_list:
                recommendations[direction.critique.recommendation] += 1
            print(f"   üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: Strongly Recommend: {recommendations['Strongly Recommend']}, Consider: {recommendations['Consider']}, Reject: {recommendations['Reject']}")

        print("\nüèÜ --- FINAL REPORT: PRIORITIZED RESEARCH DIRECTIONS ---")
        for direction in prioritized_list:
            print(f"\nü•á RANK #{direction.rank}: {direction.title}")
            print(f"   üìä SCORE: {direction.critique.final_score:.2f}/10")
            print(f"   üéØ RECOMMENDATION: {direction.critique.recommendation}")
            print(f"   üìù Description: {direction.description}")
            print(f"   üìö Supporting Papers: {', '.join(direction.supporting_papers)}")
            print(f"   üîç CRITIQUE:")
            print(f"     ‚úÖ Novelty: {direction.critique.novelty_score}/10")
            print(f"     üéØ Impact: {direction.critique.impact_score}/10") 
            print(f"     ‚ö° Feasibility: {direction.critique.feasibility_score}/10")
            
            # –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫
            strengths_text = '; '.join(direction.critique.strengths)
            weaknesses_text = '; '.join(direction.critique.weaknesses)
            
            print(f"     üí™ Strengths:")
            for strength in direction.critique.strengths:
                print(f"        ‚Ä¢ {strength}")
            print(f"     ‚ö†Ô∏è Weaknesses:")  
            for weakness in direction.critique.weaknesses:
                print(f"        ‚Ä¢ {weakness}")
    else:
        print("   ‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.")
    
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
    print(f"   ‚Ä¢ –ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π: {GRAPH_FILE}")
    print(f"   ‚Ä¢ –û—Ç—á–µ—Ç —Å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏: {REPORT_FILE}")
    print("üîÑ –î–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∞ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ FORCE_REBUILD = True")